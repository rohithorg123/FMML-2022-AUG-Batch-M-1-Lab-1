{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rohithorg123/FMML-2022-AUG-Batch-M-1-Lab-1/blob/main/Module_7_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6WbuhKIaj8v"
      },
      "source": [
        "#Module 7 Project: Movie Recommendation Engine"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Module Coordinator: Nikunj Nawal`"
      ],
      "metadata": {
        "id": "nY0Fv5ynQ03G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this module, we will try to create a Movie Recommendation System using different unsupervised learning techniques."
      ],
      "metadata": {
        "id": "yb8Th0QVRLWU"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMBdXlbjbEf4"
      },
      "source": [
        "# dataset download\n",
        "\n",
        "# !wget https://files.grouplens.org/datasets/movielens/ml-100k.zip\n",
        "!wget https://files.grouplens.org/datasets/movielens/ml-latest-small.zip\n",
        "!unzip ml-latest-small.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUjEAuW0eHbZ"
      },
      "source": [
        "## Clustering based recommendation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtuurMa5aeiX"
      },
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy.sparse import csr_matrix\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import itertools\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZ1H-jLmcT49"
      },
      "source": [
        "# Import the Movies dataset\n",
        "movies = pd.read_csv('ml-latest-small/movies.csv')\n",
        "movies.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dBSzAp5cW5w"
      },
      "source": [
        "# Import the ratings dataset\n",
        "ratings = pd.read_csv('ml-latest-small/ratings.csv')\n",
        "ratings.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRRxx1bVcZAr"
      },
      "source": [
        "# Print the number of records and the total number of movies\n",
        "print('The dataset contains: ', len(ratings), ' ratings of ', len(movies), ' movies.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ec8LbfpxcfyB"
      },
      "source": [
        "### Romance versus Science Fiction\n",
        "\n",
        "We will start by considering a subset of users and discovering what are their favourite genre. We will do this by defining a function that will calculate each userâ€™s average rating for all science fiction and romance movies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEiAml47ca7O"
      },
      "source": [
        "# Function to get the genre ratings\n",
        "def get_genre_ratings(ratings, movies, genres, column_names):\n",
        "    genre_ratings = pd.DataFrame()\n",
        "    for genre in genres:        \n",
        "        genre_movies = movies[movies['genres'].str.contains(genre) ]\n",
        "        avg_genre_votes_per_user = ratings[ratings['movieId'].isin(genre_movies['movieId'])].loc[:, ['userId', 'rating']].groupby(['userId'])['rating'].mean().round(2)\n",
        "        \n",
        "        genre_ratings = pd.concat([genre_ratings, avg_genre_votes_per_user], axis=1)\n",
        "        \n",
        "    genre_ratings.columns = column_names\n",
        "    return genre_ratings# Calculate the average rating of romance and scifi movies\n",
        "genre_ratings = get_genre_ratings(ratings, movies, ['Romance', 'Sci-Fi'], ['avg_romance_rating', 'avg_scifi_rating'])\n",
        "genre_ratings.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79b5Uz3KcmSu"
      },
      "source": [
        "In order to have a more delimited subset of people to study, we are going to bias our grouping to only get ratings from those users that like either romance or science fiction movies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdiswAdbceds"
      },
      "source": [
        "# Function to get the biased dataset\n",
        "def bias_genre_rating_dataset(genre_ratings, score_limit_1, score_limit_2):\n",
        "    biased_dataset =    genre_ratings[((genre_ratings['avg_romance_rating'] < score_limit_1 - 0.2) & (genre_ratings['avg_scifi_rating'] > score_limit_2)) | ((genre_ratings['avg_scifi_rating'] < score_limit_1) & (genre_ratings['avg_romance_rating'] > score_limit_2))]\n",
        "    biased_dataset = pd.concat([biased_dataset[:300], genre_ratings[:2]])\n",
        "    biased_dataset = pd.DataFrame(biased_dataset.to_records())\n",
        "    return biased_dataset\n",
        "\n",
        "# Bias the dataset\n",
        "biased_dataset = bias_genre_rating_dataset(genre_ratings, 3.2, 2.5)\n",
        "\n",
        "# Printing the resulting number of records & the head of the dataset\n",
        "print( \"Number of records: \", len(biased_dataset))\n",
        "biased_dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLCoVvz0ctAh"
      },
      "source": [
        "# Defining the scatterplot drawing function\n",
        "def draw_scatterplot(x_data, x_label, y_data, y_label):\n",
        "    fig = plt.figure(figsize=(8,8))\n",
        "    ax = fig.add_subplot(111)    \n",
        "    plt.xlim(0, 5)\n",
        "    plt.ylim(0, 5)\n",
        "    ax.set_xlabel(x_label)\n",
        "    ax.set_ylabel(y_label)\n",
        "    ax.scatter(x_data, y_data, s=30)\n",
        "    \n",
        "# Plot the scatterplot\n",
        "draw_scatterplot(biased_dataset['avg_scifi_rating'],'Avg scifi rating', biased_dataset['avg_romance_rating'], 'Avg romance rating')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRfknsM1c1Ur"
      },
      "source": [
        "# Let's turn our dataset into a list\n",
        "X = biased_dataset[['avg_scifi_rating','avg_romance_rating']].values\n",
        "\n",
        "# Import KMeans\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Create an instance of KMeans to find two clusters\n",
        "kmeans_1 = KMeans(n_clusters=2)\n",
        "\n",
        "# Use fit_predict to cluster the dataset\n",
        "predictions = kmeans_1.fit_predict(X)\n",
        "\n",
        "# Defining the cluster plotting function\n",
        "def draw_clusters(biased_dataset, predictions, cmap='viridis'):\n",
        "    fig = plt.figure(figsize=(8,8))\n",
        "    ax = fig.add_subplot(111)\n",
        "    plt.xlim(0, 5)\n",
        "    plt.ylim(0, 5)\n",
        "    ax.set_xlabel('Avg scifi rating')\n",
        "    ax.set_ylabel('Avg romance rating')\n",
        "    clustered = pd.concat([biased_dataset.reset_index(), pd.DataFrame({'group':predictions})], axis=1)\n",
        "    plt.scatter(clustered['avg_scifi_rating'], clustered['avg_romance_rating'], c=clustered['group'], s=20, cmap=cmap)\n",
        "    \n",
        "# Plot\n",
        "draw_clusters(biased_dataset, predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tVnggzlc-rg"
      },
      "source": [
        "# Create an instance of KMeans to find three clusters\n",
        "kmeans_2 = KMeans(n_clusters=3)\n",
        "# Use fit_predict to cluster the dataset\n",
        "predictions_2 = kmeans_2.fit_predict(X)\n",
        "# Plot\n",
        "draw_clusters(biased_dataset, predictions_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xi5ObyC0dLle"
      },
      "source": [
        "It is evident now that the science-fiction rating has started to come into play:\n",
        "\n",
        "    People who like sci-fi and romance modeately belong to the green group.\n",
        "    People who like scifi but not romance belong to the yello group.\n",
        "    People who like romance but not sci-fi belong to the purple group."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5w_-jzld_rM"
      },
      "source": [
        "Finding optimal clusters using silhouette score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyzcralUdFxj"
      },
      "source": [
        "# Selecting our dataset to study\n",
        "df = biased_dataset[['avg_scifi_rating','avg_romance_rating']]\n",
        "\n",
        "# Choose the range of k values to test.\n",
        "# We added a stride of 5 to improve performance. We don't need to calculate the error for every k value\n",
        "possible_k_values = range(2, len(X)+1, 5)\n",
        "\n",
        "# Define function to calculate the clustering errors\n",
        "def clustering_errors(k, data):\n",
        "    kmeans = KMeans(n_clusters=k).fit(data)\n",
        "    predictions = kmeans.predict(data)\n",
        "    #cluster_centers = kmeans.cluster_centers_\n",
        "    # errors = [mean_squared_error(row, cluster_centers[cluster]) for row, cluster in zip(data.values, predictions)]\n",
        "    # return sum(errors)\n",
        "    silhouette_avg = silhouette_score(data, predictions)\n",
        "    return silhouette_avg\n",
        "\n",
        "# Calculate error values for all k values we're interested in\n",
        "errors_per_k = [clustering_errors(k, X) for k in possible_k_values]\n",
        "\n",
        "# Plot the each value of K vs. the silhouette score at that value\n",
        "fig, ax = plt.subplots(figsize=(16, 6))\n",
        "plt.plot(possible_k_values, errors_per_k)\n",
        "\n",
        "# Ticks and grid\n",
        "xticks = np.arange(min(possible_k_values), max(possible_k_values)+1, 5.0)\n",
        "ax.set_xticks(xticks, minor=False)\n",
        "ax.set_xticks(xticks, minor=True)\n",
        "ax.xaxis.grid(True, which='both')\n",
        "yticks = np.arange(round(min(errors_per_k), 2), max(errors_per_k), .05)\n",
        "ax.set_yticks(yticks, minor=False)\n",
        "ax.set_yticks(yticks, minor=True)\n",
        "ax.yaxis.grid(True, which='both')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j02Q87YjepyP"
      },
      "source": [
        "We will chose the K = 7 as it is the one that yields the best score and will be easier to visualize.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lV1yJyAbeWHC"
      },
      "source": [
        "# Create an instance of KMeans to find seven clusters\n",
        "kmeans_4 = KMeans(n_clusters=7)\n",
        "# Use fit_predict to cluster the dataset\n",
        "predictions_4 = kmeans_4.fit_predict(X)\n",
        "# Plot\n",
        "draw_clusters(biased_dataset, predictions_4, cmap='Accent')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bqqi2h3FfaXs"
      },
      "source": [
        "Up to now, we have only analyzed romance and science-fiction movies. Let us see what happens when adding other genre to our analysis by adding Action movies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wv4ykXxweyrf"
      },
      "source": [
        "# Select our biased dataset and add action genre\n",
        "biased_dataset_3_genres = get_genre_ratings(ratings, movies, ['Romance','Sci-Fi', 'Action'],                                          \n",
        "['avg_romance_rating', 'avg_scifi_rating', 'avg_action_rating'])\n",
        "# Drop null values\n",
        "biased_dataset_3_genres = bias_genre_rating_dataset(biased_dataset_3_genres, 3.2, 2.5).dropna()\n",
        "# Print the number of records and the head of our dataset\n",
        "print( \"Number of records: \", len(biased_dataset_3_genres))\n",
        "biased_dataset_3_genres.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmxE7OEbfjgH"
      },
      "source": [
        "# Turn dataset into a list\n",
        "X_with_action = biased_dataset_3_genres[['avg_scifi_rating','avg_romance_rating','avg_action_rating']].values# Create an instance of KMeans to find seven clusters\n",
        "kmeans_5 = KMeans(n_clusters=7)\n",
        "# Use fit_predict to cluster the dataset\n",
        "predictions_5 = kmeans_5.fit_predict(X_with_action)\n",
        "# Define 3d plotting function\n",
        "def draw_clusters_3d(biased_dataset_3, predictions):\n",
        "    fig = plt.figure(figsize=(8,8))\n",
        "    ax = fig.add_subplot(111)\n",
        "    plt.xlim(0, 5)\n",
        "    plt.ylim(0, 5)\n",
        "    ax.set_xlabel('Avg scifi rating')\n",
        "    ax.set_ylabel('Avg romance rating')\n",
        "    clustered = pd.concat([biased_dataset_3.reset_index(), pd.DataFrame({'group':predictions})], axis=1)\n",
        "    colors = itertools.cycle(plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"])\n",
        "    for g in clustered.group.unique():\n",
        "        color = next(colors)\n",
        "        for index, point in clustered[clustered.group == g].iterrows():\n",
        "            if point['avg_action_rating'].astype(float) > 3: \n",
        "                size = 50\n",
        "            else:\n",
        "                size = 15\n",
        "            plt.scatter(point['avg_scifi_rating'], \n",
        "                        point['avg_romance_rating'], \n",
        "                        s=size, \n",
        "                        color=color)# Plot\n",
        "draw_clusters_3d(biased_dataset_3_genres, predictions_5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Upy7G0Ta5MH"
      },
      "source": [
        "The size of the dots represent the ratings of the action movies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzfFJTMJbGCk"
      },
      "source": [
        "### Taking users into consideration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JahmtB_0f5fd"
      },
      "source": [
        "Once we have seen and understood how the K-Means algorithm group the users by their movie genre preferences, we are going to take a bigger picture of the dataset and explore how users rate individual movies.\n",
        "\n",
        "To do so, we will subset the dataset by â€˜useridâ€™ vs â€˜user ratingâ€™ as follows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oy6yk-kxfuxC"
      },
      "source": [
        "# Merge the two tables then pivot so we have Users X Movies dataframe\n",
        "ratings_title = pd.merge(ratings, movies[['movieId', 'title']], on='movieId' )\n",
        "user_movie_ratings = pd.pivot_table(ratings_title, index='userId', columns= 'title', values='rating')\n",
        "\n",
        "# Print he number of dimensions and a subset of the dataset\n",
        "print('dataset dimensions: ', user_movie_ratings.shape, '\\n\\nSubset example:')\n",
        "user_movie_ratings.iloc[:6, :10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSB8eHPqf-Xc"
      },
      "source": [
        "# Define Function to get the most rated movies\n",
        "def get_most_rated_movies(user_movie_ratings, max_number_of_movies):\n",
        "    # 1- Count\n",
        "    user_movie_ratings = user_movie_ratings.append(user_movie_ratings.count(), ignore_index=True)\n",
        "    # 2- sort\n",
        "    user_movie_ratings_sorted = user_movie_ratings.sort_values(len(user_movie_ratings)-1, axis=1, ascending=False)\n",
        "    user_movie_ratings_sorted = user_movie_ratings_sorted.drop(user_movie_ratings_sorted.tail(1).index)\n",
        "    # 3- slice\n",
        "    most_rated_movies = user_movie_ratings_sorted.iloc[:, :max_number_of_movies]\n",
        "    return most_rated_movies\n",
        "\n",
        "# Define function to get the user who rate a movie the most\n",
        "def get_users_who_rate_the_most(most_rated_movies, n_users):\n",
        "    most_rated_movies['num_ratings'] = - most_rated_movies.isnull().sum(axis=1)\n",
        "    most_rated_movies = most_rated_movies.sort_values(by=['num_ratings'], ascending=False)\n",
        "    most_rated_movies = most_rated_movies.iloc[:n_users, :]\n",
        "    most_rated_movies = most_rated_movies.drop('num_ratings', axis=1)\n",
        "    return most_rated_movies\n",
        "\n",
        "# Define the sorting by rating function\n",
        "def sort_by_rating_density(user_movie_ratings, n_movies, n_users):\n",
        "    most_rated_movies = get_most_rated_movies(user_movie_ratings, n_movies)\n",
        "    most_rated_movies = get_users_who_rate_the_most(most_rated_movies, n_users)\n",
        "    return most_rated_movies\n",
        "    \n",
        "# choose the number of movies and users and sort\n",
        "n_movies = 30\n",
        "n_users = 18\n",
        "most_rated_movies_users_selection = sort_by_rating_density(user_movie_ratings, n_movies, n_users)\n",
        "\n",
        "# Print the result\n",
        "# print(most_rated_movies_users_selection)\n",
        "print('dataset dimensions: ', most_rated_movies_users_selection.shape)\n",
        "print(most_rated_movies_users_selection.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMmuIqVxgGAN"
      },
      "source": [
        "# Define the plotting heatmap function\n",
        "def draw_movies_heatmap(most_rated_movies_users_selection, axis_labels=True):\n",
        "    \n",
        "    fig = plt.figure(figsize=(15,4))\n",
        "    ax = plt.gca()\n",
        "    \n",
        "    # Draw heatmap\n",
        "    heatmap = ax.imshow(most_rated_movies_users_selection,  interpolation='nearest', vmin=0, vmax=5, aspect='auto')\n",
        "    if axis_labels:\n",
        "        ax.set_yticks(np.arange(most_rated_movies_users_selection.shape[0]) , minor=False)\n",
        "        ax.set_xticks(np.arange(most_rated_movies_users_selection.shape[1]) , minor=False)\n",
        "        ax.invert_yaxis()\n",
        "        ax.xaxis.tick_top()\n",
        "        labels = most_rated_movies_users_selection.columns.str[:40]\n",
        "        ax.set_xticklabels(labels, minor=False)\n",
        "        ax.set_yticklabels(most_rated_movies_users_selection.index, minor=False)\n",
        "        plt.setp(ax.get_xticklabels(), rotation=90)\n",
        "    else:\n",
        "        ax.get_xaxis().set_visible(False)\n",
        "        ax.get_yaxis().set_visible(False)\n",
        "    \n",
        "    ax.grid(False)\n",
        "    ax.set_ylabel('User id')# Separate heatmap from color bar\n",
        "    divider = make_axes_locatable(ax)\n",
        "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)# Color bar\n",
        "    cbar = fig.colorbar(heatmap, ticks=[5, 4, 3, 2, 1, 0], cax=cax)\n",
        "    cbar.ax.set_yticklabels(['5 stars', '4 stars','3 stars','2 stars','1 stars','0 stars'])\n",
        "    plt.show()# Print the heatmap\n",
        "draw_movies_heatmap(most_rated_movies_users_selection)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTdEC1i5hS-M"
      },
      "source": [
        "# Pivot the dataset and choose the first 1000 movies\n",
        "user_movie_ratings =  pd.pivot_table(ratings_title, index='userId', columns= 'title', values='rating')\n",
        "most_rated_movies_1k = get_most_rated_movies(user_movie_ratings, 1000).replace(np.nan, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMGpCM8SikyB"
      },
      "source": [
        "from scipy import sparse\n",
        "\n",
        "# Conversion to sparse csr matrix\n",
        "sparse_ratings = sparse.csr_matrix(most_rated_movies_1k.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5j6M6TL7j5B7"
      },
      "source": [
        "### Large Scale Clustering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcvHmGnZipva"
      },
      "source": [
        "# 20 clusters\n",
        "predictions = KMeans(n_clusters=20, algorithm='full').fit_predict(sparse_ratings)\n",
        "# Select the mas number of users and movies heatmap cluster\n",
        "# Cluster and print some of them\n",
        "clustered = pd.concat([get_most_rated_movies(user_movie_ratings, 1000).reset_index(), pd.DataFrame({'group':predictions})], axis=1)\n",
        "# draw_movie_clusters(clustered, max_users, max_movies)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wr82U3hYo7gg"
      },
      "source": [
        "### Predictions and Recommendation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSx-kAa2j9rJ"
      },
      "source": [
        "# Pick a cluster ID from the clusters above\n",
        "cluster_number = 11\n",
        "# Let's filter to only see the region of the dataset with the most number of values \n",
        "n_users = 75\n",
        "n_movies = 300\n",
        "cluster = clustered[clustered.group == cluster_number].drop(['index', 'group'], axis=1)\n",
        "print(cluster)\n",
        "# Sort and print the cluster\n",
        "cluster = sort_by_rating_density(cluster, n_movies, n_users)\n",
        "draw_movies_heatmap(cluster, axis_labels=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1H3vAespxSf"
      },
      "source": [
        "# Fill in the name of the column/movie. e.g. 'Forrest Gump (1994)'\n",
        "movie_name = \"Matrix, The (1999)\"\n",
        "cluster[movie_name].mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eaBbILNpsWA"
      },
      "source": [
        "# The average rating of 20 movies as rated by the users in the cluster\n",
        "cluster.mean().head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VKa3_ZiyeGg"
      },
      "source": [
        "# Predict for a specific user\n",
        "\n",
        "# Pick a user ID from the dataset\n",
        "user_id = 19\n",
        "# Get all this user's ratings\n",
        "user_2_ratings  = cluster.loc[user_id, :]\n",
        "# Which movies did they not rate? \n",
        "user_2_unrated_movies =  user_2_ratings[user_2_ratings.isnull()]\n",
        "# What are the ratings of these movies the user did not rate?\n",
        "avg_ratings = pd.concat([user_2_unrated_movies, cluster.mean()], axis=1, join='inner').loc[:,0]\n",
        "# Let's sort by rating so the highest rated movies are presented first\n",
        "avg_ratings.sort_values(ascending=False)[:20]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4SISKyey5rb"
      },
      "source": [
        "## Using Collaborative Filtering (With fast.ai)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eh_wLcmh4nFi"
      },
      "source": [
        "! [ -e /content ] && pip install -Uqq fastai"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GQhuh1QzBO-"
      },
      "source": [
        "from fastai.tabular.all import *\n",
        "from fastai.collab import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_C6VqNfy5KGG"
      },
      "source": [
        "### Training a model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVnoQsA25KGH"
      },
      "source": [
        "For this, we will use the [Movielens 100k data dataset](https://grouplens.org/datasets/movielens/100k/). We can download it easily and decompress it with the following function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSpPNWVo5KGI"
      },
      "source": [
        "path = untar_data(URLs.ML_100k)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ryXnRG55KGJ"
      },
      "source": [
        "The main table is in `u.data`. Since it's not a proper csv, we have to specify a few things while opening it: the tab delimiter, the columns we want to keep and their names."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aIeuyeJ5KGL"
      },
      "source": [
        "ratings = pd.read_csv(path/'u.data', delimiter='\\t', header=None,\n",
        "                      usecols=(0,1,2), names=['user','movie','rating'])\n",
        "ratings.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0FSMvM05KGP"
      },
      "source": [
        "Movie ids are not ideal to look at things, so we load the corresponding movie id to the title that is in the table `u.item`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3y_V-da5KGQ"
      },
      "source": [
        "movies = pd.read_csv(path/'u.item',  delimiter='|', encoding='latin-1',\n",
        "                     usecols=(0,1), names=('movie','title'), header=None)\n",
        "movies.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g11IQqmd5KGR"
      },
      "source": [
        "Next we merge it to our ratings table:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZmPTq285KGT"
      },
      "source": [
        "ratings = ratings.merge(movies)\n",
        "ratings.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PksImdkw5KGV"
      },
      "source": [
        "We can then build a `DataLoaders` object from this table. By default, it takes the first column for user, the second column for the item (here our movies) and the third column for the ratings. We need to change the value of `item_name` in our case, to use the titles instead of the ids:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OV-H74uh5KGV"
      },
      "source": [
        "dls = CollabDataLoaders.from_df(ratings, item_name='title', bs=64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyK3Sy9L5KGW"
      },
      "source": [
        "In all applications, when the data has been assembled in a `DataLoaders`, you can have a look at it with the `show_batch` method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U07N7Tue5KGW"
      },
      "source": [
        "dls.show_batch()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMiAsrvW5KGY"
      },
      "source": [
        "fastai can create and train a collaborative filtering model by using `collab_learner`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rXePHqS5KGY"
      },
      "source": [
        "learn = collab_learner(dls, n_factors=50, y_range=(0, 5.5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58blYToq5KGY"
      },
      "source": [
        "It uses a simple dot product model with 50 latent factors. To train it using the 1cycle policy, we just run this command:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ze2sG7Pd5KGZ"
      },
      "source": [
        "learn.fit_one_cycle(5, 5e-3, wd=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXpc4wrh5KGZ"
      },
      "source": [
        "Here's [some benchmarks](https://www.librec.net/release/v1.3/example.html) on the same dataset for the popular Librec system for collaborative filtering. They show best results based on RMSE of 0.91 (scroll down to the 100k dataset), which corresponds to an MSE of `0.91**2 = 0.83`. So in less than a minute, we got pretty good results!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNlR8PdV5KGa"
      },
      "source": [
        "### Interpretation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Kp8v39y5KGa"
      },
      "source": [
        "Let's analyze the results of our previous model. We will keep the 1000 most rated movies for this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOBM1Dmn5KGa"
      },
      "source": [
        "g = ratings.groupby('title')['rating'].count()\n",
        "top_movies = g.sort_values(ascending=False).index.values[:1000]\n",
        "top_movies[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iT0TUZeV5KGa"
      },
      "source": [
        "### Movie bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTfd24Ce5KGb"
      },
      "source": [
        "Our model has learned one bias per movie, a unique number independent of users that can be interpreted as the intrinsic \"value\" of the movie. We can grab the bias of each movie in our `top_movies` list with the following command:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6_3vygf5KGb"
      },
      "source": [
        "movie_bias = learn.model.bias(top_movies, is_item=True)\n",
        "movie_bias.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LWSBSXB5KGb"
      },
      "source": [
        "Let's compare those biases with the average ratings:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-cmUxrz5KGc"
      },
      "source": [
        "mean_ratings = ratings.groupby('title')['rating'].mean()\n",
        "movie_ratings = [(b, i, mean_ratings.loc[i]) for i,b in zip(top_movies,movie_bias)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQ9NpRIY5KGd"
      },
      "source": [
        "Now let's have a look at the movies with the worst bias:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPK_3uoL5KGe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "outputId": "2f06195a-e1bc-44e1-e8cb-52690367adb4"
      },
      "source": [
        "item0 = lambda o:o[0]\n",
        "sorted(movie_ratings, key=item0)[:15]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-e801cbcf31be>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mitem0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmovie_ratings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mitem0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'movie_ratings' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyZduCwf5KGf"
      },
      "source": [
        "Or the ones with the best bias:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "la5TBDHx5KGg"
      },
      "source": [
        "sorted(movie_ratings, key=lambda o: o[0], reverse=True)[:15]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6evHscZk5KGh"
      },
      "source": [
        "There is certainly a strong correlation!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eVUbKjL5KGi"
      },
      "source": [
        "### Movie weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfpKAgSC5KGi"
      },
      "source": [
        "Now let's try to analyze the latent factors our model has learned. We can grab the weights for each movie in `top_movies` the same way as we did for the bias before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deDn2gcH5KGj"
      },
      "source": [
        "movie_w = learn.model.weight(top_movies, is_item=True)\n",
        "movie_w.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMKR-0Um5KGj"
      },
      "source": [
        "Let's try a PCA to reduce the dimensions and see if we can see what the model learned:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eaYCuVt5KGk"
      },
      "source": [
        "movie_pca = movie_w.pca(3)\n",
        "movie_pca.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gP8YQMM55KGl"
      },
      "source": [
        "fac0,fac1,fac2 = movie_pca.t()\n",
        "movie_comp = [(f, i) for f,i in zip(fac0, top_movies)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRscjh3K5KGl"
      },
      "source": [
        "Here are the highest score on the first dimension:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L74WWxjA5KGl"
      },
      "source": [
        "sorted(movie_comp, key=itemgetter(0), reverse=True)[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMS1ASOm5KGm"
      },
      "source": [
        "And the worst:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDxgTWUW5KGm"
      },
      "source": [
        "sorted(movie_comp, key=itemgetter(0))[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXZyCaJL5KGn"
      },
      "source": [
        "Same thing for our second dimension:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JR6BnTn5KGn"
      },
      "source": [
        "movie_comp = [(f, i) for f,i in zip(fac1, top_movies)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xj4_P5dS5KGn"
      },
      "source": [
        "sorted(movie_comp, key=itemgetter(0), reverse=True)[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28K9s9Rp5KGo"
      },
      "source": [
        "sorted(movie_comp, key=itemgetter(0))[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZAynsWP5KGo"
      },
      "source": [
        "And we can even plot the movies according to their scores on those dimensions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHtORCAp5KGp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "a1f3f8e5-afc7-43ef-8e4f-76d1d614c420"
      },
      "source": [
        "idxs = np.random.choice(len(top_movies), 50, replace=False)\n",
        "idxs = list(range(50))\n",
        "X = fac0[idxs]\n",
        "Y = fac2[idxs]\n",
        "plt.figure(figsize=(15,15))\n",
        "plt.scatter(X, Y)\n",
        "for i, x, y in zip(top_movies[idxs], X, Y):\n",
        "    plt.text(x,y,i, color=np.random.rand(3)*0.7, fontsize=11)\n",
        "plt.show()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c954b6800fe6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0midxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_movies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0midxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfac0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfac2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ]
    }
  ]
}